{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis using Singular Value Decomposition\n",
    "\n",
    "The majority of Machine Learning problems include an enormous amount of features for each sample point. This both drastically increases the training time and makes finding the reasonable solution infeasible (the curse of dimensionality). Thus, we woud love to devise a way of representing the original training set using smaller number of dimensions, which we can do using Singular Value Decomposition.\n",
    "\n",
    "### Singular Value Decomposition\n",
    "\n",
    "Eigendecomposition $A = V \\Lambda V^{-1}$ is only applicable to square matrices that have enough linearly independent eigenvectors. Even if the matrix is diagonalizable,  $V$ does not always have a nice property of orthogonality (it does so only if $A$ is symmetric). Singular Value Decomposition, on the other hand, is a much nicer decomposition technique applicable to all sorts of matrices that provides us with orthonormal bases. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
