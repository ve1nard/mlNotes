{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis using Singular Value Decomposition\n",
    "\n",
    "The majority of Machine Learning problems include an enormous amount of features for each sample point. This both drastically increases the training time and makes finding the reasonable solution infeasible (the curse of dimensionality). Thus, we woud love to devise a way of representing the original training set using smaller number of dimensions, which we can do using Singular Value Decomposition.\n",
    "\n",
    "### Singular Value Decomposition\n",
    "\n",
    "Eigendecomposition $A = V \\Lambda V^{-1}$ is only applicable to square matrices that have enough linearly independent eigenvectors. Even if the matrix is diagonalizable,  $V$ does not always have a nice property of orthogonality (it does so only if $A$ is symmetric). Singular Value Decomposition, on the other hand, is a much nicer decomposition technique applicable to all sorts of matrices that provides us with orthonormal bases. \n",
    "\n",
    "Let $A$ be a real $m \\times n$ matrix of rank $r$. Write $I_r$ for the $r \\times r$ identity matrix. A singular value decomposition (SVD) of $A$ is a factorization:\n",
    "\n",
    "$$\n",
    "A = U_r \\Sigma_r V_r^T,\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $U_r$ is an $m \\times r$ matrix such that  \n",
    "  $$\n",
    "  U_r^T U_r = I_r,\n",
    "  $$\n",
    "\n",
    "- $V$ is an $n \\times r$ matrix such that  \n",
    "  $$\n",
    "  V_r^T V_r = I_r,\n",
    "  $$\n",
    "\n",
    "- $\\Sigma_r$ is an $r \\times r$ diagonal matrix  \n",
    "  $$\n",
    "  \\Sigma_r =\n",
    "  \\begin{pmatrix}\n",
    "  \\sigma_1 & 0 & \\cdots & 0 \\\\\n",
    "  0 & \\sigma_2 & \\cdots & 0 \\\\\n",
    "  \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "  0 & 0 & \\cdots & \\sigma_r\n",
    "  \\end{pmatrix},\n",
    "  $$\n",
    "\n",
    "  where $\\sigma_1 \\geq \\sigma_2 \\geq \\cdots \\geq \\sigma_r$ are strictly positive.\n",
    "\n",
    "This is a definition of the \"reduced\" SVD, where a set $u_1, \\dots, u_r$ is an orthonormal basis for the column space of $A$, while $v_1, v_r$ is an orthonormal basis for the row space of $A$. We can extend it by using the remaining $n-r$ $v$'s and $m-r$ $u$'s that form the nullspace $N(A)$ and the left nullspace $N(A^T)$, respectively, to get the full SVD $A = U \\Sigma V^T$, where $\\Sigma$ is an $m \\times n$ matrix constructed from $\\Sigma_r$ by adding extra $m-r$ zero rows and $n-r$ zero columns. This decomposition can be rewritten as a sum of r rank-one matrices:\n",
    "\n",
    "$$\n",
    "A = U \\Sigma V^T = u_1 \\sigma_1 v_1^T + \\dots u_r \\sigma_1 v_r^T\n",
    "$$\n",
    "\n",
    "Let's see how this decomposition be obtained (adapted from [MIT notes](https://math.mit.edu/classes/18.095/2016IAP/lec2/SVD_Notes.pdf)):\n",
    "\n",
    "Using the SVD of $A$, we can represent $A^TA$ as $A^TA = (U \\Sigma V^T)^T (U \\Sigma V^T) = V \\Sigma^T U^T U \\Sigma V^T = V \\Sigma^T \\Sigma V^T$, which is the eigendecomposition of $A^TA$, where $v$'s are orthonormal, and each $\\sigma^2$ is $\\lambda(A^TA)$. Now, since $Av_i =\\sigma_i u_i$, we can find $u$'s as $u_i = \\frac{Av_i}{\\sigma_i}$. To finish the derivation, we need to show that $u$'s are also orthonormal:\n",
    "$$\n",
    "||u_i||^2 = u_i^Tu_i = \\frac{v_i^TA^TAv_i}{\\sigma_i^2} = \\frac{v_i^T\\sigma_i^2v_i}{\\sigma_i^2} = v_I^Tv_i = 1\n",
    "$$\n",
    "and \n",
    "\n",
    "$$\n",
    "u_i^Tu_j = \\frac{(Av_i)^T}{\\sigma_i} \\frac{(Av_j)}{\\sigma_j} = \\frac{v_i^TA^TAv_j}{\\sigma_i \\sigma_j} = \\frac{v_i^T\\sigma_j^2v_j}{\\sigma_i \\sigma_j} = \\frac{\\sigma_j^2}{\\sigma_i \\sigma_j}v_i^Tv_j = 0\n",
    "$$\n",
    "\n",
    "Next, we complete the $v$’s and $u$’s to n $v$’s and m $u$’s with any orthonormal bases for the nullspaces $N(A)$ and $N(A^T)$, thereby completing the derivation.\n",
    "\n",
    "We will use SVD to find the best low-rank approximation of a given matrix $A$. To know how well some matrix approximates the given one, we want to introduce the Frobenius matrix norm for an $m \\times n$ matrix denoted as $||A||_F$ and defined as\n",
    "\n",
    "$$\n",
    "||A||_F = \\sqrt{\\sum\\limits_{i=1}^m\\sum\\limits_{j=1}^na_{ij}^2} = \\sqrt{trace(A^TA)}\n",
    "$$\n",
    "\n",
    "It is important to mention that this norm is a function of the singular values of $A = U \\Sigma V^T$:\n",
    "\n",
    "$$\n",
    "||A||_F = \\sqrt{trace(A^TA)} = \\sqrt{trace((U\\Sigma V^T)^T(U\\Sigma V^T))} = \\sqrt{trace(\\Sigma^T\\Sigma)} = \\sqrt{\\sigma_1^2 + \\dots + \\sigma_r^2}\n",
    "$$\n",
    "\n",
    "To use SVD to get a low-rank approximation of matrix A, remember its decomposition as a sum of r rank-one matrices:\n",
    "\n",
    "$$\n",
    "A = u_1 \\sigma_1 v_1^T + \\dots u_r \\sigma_1 v_r^T\n",
    "$$\n",
    "\n",
    "Assuming the terms are in ascending order according to singular values, take the first k terms:\n",
    "\n",
    "$$\n",
    "A_k  = \\sum\\limits_{i=1}^k\\sigma_i u_i v_i^T\n",
    "$$\n",
    "\n",
    "which gives us a rank-k matrix with k singular values. It follows that $A - A_k = \\sum\\limits_{i=k+1}^r\\sigma_i u_i v_i^T$ has singular values $\\sigma_{k+1} \\dots \\sigma_{r}$. Thus, the Frobenius norm of $A - A_k$ is:\n",
    "\n",
    "$$\n",
    "\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
