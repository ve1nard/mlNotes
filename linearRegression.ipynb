{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Linear Regression Model\n",
    "The linear regression model is typically written as:\n",
    "\n",
    "$$\n",
    "Y = X\\beta + \\epsilon\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ Y $ is the dependent variable (observed outcomes),\n",
    "- $ X $ is the matrix of independent variables (predictors),\n",
    "- $ \\beta $ is the true vector of coefficients (parameters to be estimated),\n",
    "- $ \\hat{\\beta} $ is the estimation of the true vector of coefficients $ \\beta $\n",
    "- $ \\epsilon $ is the error term (assumed to be independent and identically distributed).\n",
    "\n",
    "Minimizing Mean Squared Error $\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^n (Y_i - X_i\\hat{\\beta})^2$ provides a solution to this problem. In matrix form it can be written as $\\hat{\\beta} = \\arg\\min_{\\hat{\\beta}}\\| Y - X \\hat{\\beta} \\|_2^2$\n",
    "\n",
    "#### 2. MSE as MLE under the assumption of Normality for $ \\epsilon $\n",
    "The mean square error (MSE) loss in linear regression can be interpreted as a specific implementation of maximum likelihood estimation (MLE) under the assumption that the error term follows a normal distribution $ \\epsilon \\sim \\mathcal{N}(0, \\sigma^2) $ with mean 0 and variance $ \\sigma^2 $.\n",
    "\n",
    "This implies that $ Y | X \\sim \\mathcal{N}(X\\hat{\\beta}, \\sigma^2) $, meaning the conditional distribution of $ Y $ given $ X $ is normally distributed with mean $ X\\hat{\\beta} $ and variance $ \\sigma^2 $.\n",
    "\n",
    "In MLE, we aim to find the parameter $ \\hat{\\beta} $ that maximizes the likelihood of the observed data.\n",
    "\n",
    "The likelihood function for the data $ \\{Y_i, X_i\\}_{i=1}^n $ is:\n",
    "\n",
    "$$\n",
    "L(\\hat{\\beta}, \\sigma^2) = \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(Y_i - X_i\\hat{\\beta})^2}{2\\sigma^2}\\right)\n",
    "$$\n",
    "\n",
    "Taking the natural logarithm to simplify (log-likelihood):\n",
    "\n",
    "$$\n",
    "\\ell(\\hat{\\beta}, \\sigma^2) = -\\frac{n}{2} \\log(2\\pi) - \\frac{n}{2} \\log(\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^n (Y_i - X_i\\hat{\\beta})^2\n",
    "$$\n",
    "\n",
    "To maximize the log-likelihood with respect to $ \\hat{\\beta} $:\n",
    "- The terms $ -\\frac{n}{2} \\log(2\\pi) $ and $ -\\frac{n}{2} \\log(\\sigma^2) $ are constants with respect to $ \\hat{\\beta} $, so they can be ignored.\n",
    "- The remaining term to maximize (or equivalently minimize its negative) is:\n",
    "\n",
    "$$\n",
    "\\frac{1}{2\\sigma^2} \\sum_{i=1}^n (Y_i - X_i\\hat{\\beta})^2\n",
    "$$\n",
    "\n",
    "Minimizing this is equivalent to minimizing the sum of squared errors (SSE):\n",
    "\n",
    "$$\n",
    "\\text{SSE} = \\sum_{i=1}^n (Y_i - X_i\\hat{\\beta})^2\n",
    "$$\n",
    "\n",
    "Dividing by $ n $ gives the mean squared error (MSE):\n",
    "\n",
    "$$\n",
    "\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^n (Y_i - X_i\\hat{\\beta})^2\n",
    "$$\n",
    "\n",
    "Thus, minimizing the MSE corresponds to maximizing the likelihood under the assumption of normally distributed errors.\n",
    "\n",
    "#### 3. Analytical solution\n",
    "Our loss function is:\n",
    "\n",
    "$$\n",
    "RSS(\\hat{\\beta}) = (y - X\\hat{\\beta})^T (y - X\\hat{\\beta})\n",
    "$$\n",
    "\n",
    "Expanding this and using the fact that $(u - v)^T = u^T - v^T$, we have:\n",
    "\n",
    "$$\n",
    "RSS(\\hat{\\beta}) = y^T y - y^T X\\hat{\\beta} - \\hat{\\beta}^T X^T y + \\hat{\\beta}^T X^T X \\hat{\\beta}.\n",
    "$$\n",
    "\n",
    "Noting that $y^T X\\hat{\\beta}$ is a scalar, and for any scalar $r \\in \\mathbb{R}$ we have $r = r^T$, it follows that:\n",
    "\n",
    "$$\n",
    "y^T X\\hat{\\beta} = (y^T X\\hat{\\beta})^T = \\hat{\\beta}^T X^T y.\n",
    "$$\n",
    "\n",
    "So, all together:\n",
    "\n",
    "$$\n",
    "RSS(\\hat{\\beta}) = y^T y - 2\\hat{\\beta}^T X^T y + \\hat{\\beta}^T X^T X \\hat{\\beta}.\n",
    "$$\n",
    "\n",
    "*Note: If you are not familiar with matrix calculus, see this [cookbook](https://atmos.washington.edu/~dennis/MatrixCalculus.pdf) by Barnes (2006) with detailed derivations before proceeding with the next steps.*\n",
    "\n",
    "Now we'll differentiate $RSS(\\hat{\\beta})$ with respect to $\\hat{\\beta}$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial RSS}{\\partial \\hat{\\beta}} = \\frac{\\partial}{\\partial \\hat{\\beta}} y^T y - 2 \\frac{\\partial}{\\partial \\hat{\\beta}} \\hat{\\beta}^T X^T y + \\frac{\\partial}{\\partial \\hat{\\beta}} \\hat{\\beta}^T X^T X \\hat{\\beta}.\n",
    "$$\n",
    "\n",
    "Since $y^T y$ is constant with respect to $\\hat{\\beta}$, its derivative is 0. The other terms simplify as follows:\n",
    "- $-2 \\frac{\\partial}{\\partial \\hat{\\beta}} (\\hat{\\beta}^T X^T y) = -2 X^T y$,\n",
    "- $\\frac{\\partial}{\\partial \\hat{\\beta}} (\\hat{\\beta}^T X^T X \\hat{\\beta}) = 2 X^T X \\hat{\\beta}$.\n",
    "\n",
    "Combining these results:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial RSS}{\\partial \\hat{\\beta}} = 0 - 2 X^T y + 2 X^T X \\hat{\\beta}.\n",
    "$$\n",
    "\n",
    "We want to minimize $RSS(\\hat{\\beta})$, so we set the derivative equal to zero:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial RSS}{\\partial \\hat{\\beta}} = 0 \\implies -2 X^T y + 2 X^T X \\hat{\\beta} = 0.\n",
    "$$\n",
    "\n",
    "Simplify:\n",
    "\n",
    "$$\n",
    "X^T y - X^T X \\hat{\\beta} = 0 \\implies X^T (y - X\\hat{\\beta}) = 0.\n",
    "$$\n",
    "\n",
    "Now we assume that $X$ is full column rank, which ensures that $X^T X$ is positive definite and therefore invertible. This allows us to solve for $\\hat{\\beta}$ known as the Ordinary Least Squares (OLS) estimator:\n",
    "\n",
    "$$\n",
    "X^T y = X^T X \\hat{\\beta} \\implies \\hat{\\beta} = (X^T X)^{-1} X^T y.\n",
    "$$\n",
    "\n",
    "#### 4. Gauss-Markov theorem\n",
    "On the linear regression model defined above \n",
    "$$\n",
    "Y = X\\beta + \\epsilon\n",
    "$$\n",
    "we impose Gauss-Markov assumptions:\n",
    "- $ \\mathbb{E}(\\varepsilon_i) = 0 \\quad \\forall i $\n",
    "- $ \\text{Var}(\\varepsilon_i) = \\sigma^2 < \\infty \\quad \\forall i $\n",
    "- $ \\text{Cov}(\\varepsilon_i, \\varepsilon_j) = 0 \\quad \\forall i \\neq j $\n",
    "\n",
    "which ensure that the OLS analytical solution in the form of $\\hat{\\beta} = (X^T X)^{-1} X^T Y$ is the *Best Linear Unbiased Estimator (BLUE)*.\n",
    "\n",
    "Let's go through each part of the definition of the BLUE estimator and see why it applies to our linear regression model.\n",
    "\n",
    "The term **linear** in BLUE means that $\\hat{\\beta}$ is a linear function of the response vector $Y$. That is, each estimated coefficient in $\\hat{\\beta}$ is a weighted sum of the observed responses represented as $\\hat{\\beta} = AY$. By definition, the OLS estimator is $\\hat{\\beta} = (X^T X)^{-1} X^T Y$, making it a linear estimator.\n",
    "\n",
    "Model bias in this context refers to the variability of the estimator value based on the selection of training samples and its deviation from the true vector of coefficients. Thus, to prove the **unbiased** property, we want to show that the expected value of the estimator is equal to the true one, i.e. $\\mathbb{E}[\\hat{\\beta}] = \\beta$\n",
    "\n",
    "Taking expectations:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[\\hat{\\beta}] = \\mathbb{E}[(X^TX)^{-1} X^T Y]\n",
    "$$\n",
    "\n",
    "Since $X$ does not depend on $\\beta$, it can taken out of the expectation expression:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[\\hat{\\beta}] = (X^TX)^{-1} X^T \\mathbb{E}[Y]\n",
    "$$\n",
    "\n",
    "Substituting $ Y = X\\beta + \\epsilon $:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[\\hat{\\beta}] = (X^TX)^{-1} X^T \\mathbb{E}[X\\beta + \\epsilon] = (X^TX)^{-1} X^T (\\mathbb{E}[X\\beta] + \\mathbb{E}[\\epsilon]) = (X^TX)^{-1} X^T (X\\mathbb{E}[\\beta] + \\mathbb{E}[\\epsilon])\n",
    "$$\n",
    "\n",
    "Applying the assumption that $ \\mathbb{E}(\\varepsilon_i) = 0 \\quad \\forall i $ or equivalently $\\mathbb{E}(\\epsilon) = \\mathbf{0}$:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[\\hat{\\beta}] = (X^TX)^{-1} X^T (X\\mathbb{E}[\\beta] + \\mathbf{0}) = (X^TX)^{-1} X^T X\\mathbb{E}[\\beta] = (X^TX)^{-1} (X^T X)\\mathbb{E}[\\beta] = \\mathbb{E}[\\beta] = \\beta\n",
    "$$\n",
    "\n",
    "Thus, $\\mathbb{E}[\\hat{\\beta}] = \\beta$, proving the unbiasadness of our estimator.\n",
    "\n",
    "The **best** property states that the Ordinary Least Squares (OLS) estimator has the minimum variance among all linear unbiased estimators.\n",
    "\n",
    "The variance of $\\hat{\\beta}$ is:\n",
    "\n",
    "$$\n",
    "\\text{Var}(\\hat{\\beta}) = \\text{Var}((X^TX)^{-1} X^T Y)\n",
    "$$\n",
    "\n",
    "which, applying the fact that $(X^TX)^{-1} X^T$ is a constant matrix, can be rewritten using $\\text{Var}(AY) = A\\text{Var}(Y)A^T$ as\n",
    "\n",
    "$$\n",
    "\\text{Var}(\\hat{\\beta}) = (X^TX)^{-1} X^T Var(Y) ((X^TX)^{-1} X^T)^T\n",
    "$$\n",
    "Expanding the expression for $\\text{Var(Y)}$:\n",
    "$$\n",
    "\\text{Var(Y)} = \\text{Var}(X\\beta + \\epsilon) = \\text{Var}(X\\beta) + \\text{Var}(\\epsilon) + \\text{Cov}(X\\beta, \\epsilon) + \\text{Cov}(\\epsilon, X\\beta) \n",
    "$$\n",
    "\n",
    "Since $X\\beta$ is  a constant vector, $\\text{Var}(X\\beta)$, $\\text{Cov}(X\\beta, \\epsilon)$, and $\\text{Cov}(\\epsilon, X\\beta)$ are all zero matrices. Thus,\n",
    "\n",
    "$$\n",
    "\\text{Var(Y)} = \\text{Var}(\\epsilon)\n",
    "$$\n",
    "\n",
    "which is a matrix, whose diagonal elements are variances $\\text{Var}(e_i)$ of each of the error terms, and off-diagonal elements are their covariances $\\text{Cov}(e_i, e_j)$. Based on the assumptions of the theorem that  $\\text{Var}(\\varepsilon_i) = \\sigma^2 < \\infty \\quad \\forall i $ and $ \\text{Cov}(\\varepsilon_i, \\varepsilon_j) = 0 \\quad \\forall i \\neq j $, we realize that this is a diagonal matrix with $\\sigma^2$ as its diagonal elements. In other words,\n",
    "$$\n",
    "\\text{Var}(Y) = \\sigma^2 I\n",
    "$$\n",
    "\n",
    "Thus,\n",
    "\n",
    "$$\n",
    "\\text{Var}(\\hat{\\beta}) = (X^TX)^{-1} X^T \\sigma^2 I ((X^TX)^{-1} X^T)^T = \\sigma^2(X^TX)^{-1} X^T ((X^TX)^{-1} X^T)^T = \\sigma^2(X^TX)^{-1}\n",
    "$$\n",
    "\n",
    "To show that this variance produced by the estimator $\\hat{\\beta}$ is the smallest among all linear unbiased estimator variances, let's analyze some arbitrary linear unbiased estimator \n",
    "$\\hat{\\beta}^*$, which can be represented as \n",
    "\n",
    "$$\n",
    "\\hat{\\beta}^* = (A + (X^TX)^{-1} X^T) Y\n",
    "$$\n",
    "\n",
    "For it to be unbiased, i.e.\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[\\hat{\\beta}^*] = \\mathbb{E}[(A + (X^TX)^{-1} X^T) Y] = (A + (X^TX)^{-1} X^T)\\mathbb{E}[Y] = (A + (X^TX)^{-1} X^T)(X\\beta) = (AX + I)\\beta\n",
    "$$\n",
    "\n",
    "$AX$ must be $\\mathbf{0}$, based on which $\\hat{\\beta}^*$ can be rewritted as\n",
    "\n",
    "$$\n",
    "\\hat{\\beta}^* = \\beta + (A + (X^TX)^{-1} X^T)\\epsilon\n",
    "$$\n",
    "\n",
    "Obtaining the variance for $\\hat{\\beta}^*$:\n",
    "\n",
    "$$\n",
    "\\text{Var}(\\hat{\\beta}^*) = \\mathbb{E}[A + (X^TX)^{-1} X^T]\\epsilon \\epsilon^T [A + (X^TX)^{-1} X^T]^T =(A + (X^TX)^{-1} X^T)\\mathbb{E}(\\epsilon \\epsilon^T) (A + (X^TX)^{-1} X^T)^T \n",
    "$$\n",
    "\n",
    "Based on the assumption that $\\text{Var}(\\varepsilon_i) = \\sigma^2 < \\infty \\quad \\forall i$ and $\\text{Cov}(\\varepsilon_i, \\varepsilon_j) = 0 \\quad \\forall i \\neq j$, \n",
    "$$\n",
    "\\mathbb{E}(\\epsilon \\epsilon^T) = \\sigma^2I\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "\\text{Var}(\\hat{\\beta}^*) = \\sigma^2(AA^T + (X^TX)^{-1}) = \\sigma^2(X^TX)^{-1} + AA^T\\sigma^2 = \\text{Var}(\\hat{\\beta}) + AA^T\\sigma^2\n",
    "$$\n",
    "\n",
    "We want to measure the variance of the estimators when applied to an unknown object $a$, i.e. $\\text{Var}(a^T\\beta))$. We use the property that for a constant vector $a$ and a random vector $\\beta$, \n",
    "\n",
    "$$\n",
    "\\text{Var}(a^T\\beta) = a^T \\text{Var}(\\beta) a\n",
    "$$\n",
    "\n",
    "Thus,\n",
    "\n",
    "$$\n",
    "\\text{Var}(a^T\\hat{\\beta}) = a^T\\text{Var}(\\hat{\\beta})a\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "\\text{Var}(a^T\\hat{\\beta}^*) = a^T\\text{Var}(\\hat{\\beta}^*)a = a^T (\\text{Var}(\\hat{\\beta}) + AA^T\\sigma^2) a = a^T\\text{Var}(\\hat{\\beta})a + a^TAA^T\\sigma^2a = \\text{Var}(a^T\\hat{\\beta}) + \\sigma^2a^TAA^Ta =\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\text{Var}(a^T\\hat{\\beta}) + \\sigma^2(a^TA)(a^TA)^T\n",
    "$$\n",
    "Thus, \n",
    "\n",
    "$$\n",
    "\\text{Var}(a^T\\hat{\\beta}^*) \\geq \\text{Var}(a^T\\hat{\\beta})\n",
    "$$\n",
    "\n",
    "proving that the OLS estimator is indeed the best linear unbiased estimator.\n",
    "\n",
    "#### 5. Unstable solution and regularization\n",
    "\n",
    "In the *Analytical solution* section, we made an assumption about the invertability of $X^T X$. How would our solution bahave when this assumption fails?\n",
    "\n",
    "Let's generate the true underlying weights vector and two feature matrices: one with linearly indepent and one with correlated features (for the ease of illustration, we use two features only). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b_true: [ 1.74945474 -0.286073   -0.48456513 -2.65331856]\n",
      "b_hat: [ 1.73560828 -0.30343492 -0.47723072 -2.65414939]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "random_seed = 11\n",
    "\n",
    "n_features = 4\n",
    "n_objects = 5\n",
    "\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "# The true weights vector\n",
    "b_true = np.random.normal(size=(n_features, ))\n",
    "\n",
    "# Feature matrix with linearly independent features\n",
    "X = np.random.uniform(-7, 7, size=(n_objects, n_features))\n",
    "# Put the features into different scales (1, 10, 100, ...)\n",
    "X *= np.logspace(0, n_features-1, num=n_features)[np.newaxis, :]\n",
    "\n",
    "# The true target vector\n",
    "Y = X.dot(b_true) + np.random.normal(0, 1, size=(n_objects, ))\n",
    "\n",
    "# The analytical solution\n",
    "b_hat = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(Y)\n",
    "\n",
    "# Comparing the solution with the true vector\n",
    "print(f\"b_true: {b_true}\\nb_hat: {b_hat}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b_true: [ 1.74945474 -0.286073   -0.48456513 -2.65331856]\n",
      "b_hat: [-9.31605543e+00 -4.37812353e-01 -2.69236116e+03  2.68922311e+03]\n"
     ]
    }
   ],
   "source": [
    "# Feature matrix with colinear features obtained by copying the n-1_th feature into the n_th feature and adding a small noise\n",
    "noise = 1e-3\n",
    "X[:, -1] = X[:, -2] + np.random.uniform(-noise, noise, size=X[:, -2].shape)\n",
    "\n",
    "# The true target vector\n",
    "Y = X.dot(b_true) + np.random.normal(0, 1, size=(n_objects, ))\n",
    "\n",
    "# The analytical solution\n",
    "b_hat = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(Y)\n",
    "\n",
    "# Comparing the solution with the true vector\n",
    "print(f\"b_true: {b_true}\\nb_hat: {b_hat}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that in the case of colinear features, the solution produced is highly unstable. To fix this, we want to tweak the origianl $X^TX$ matrix to make invertible, and one of the of the ways to judge tha invertebility of the matrix is by looking at its eigenvalues: a singular matrix must have at least one zero eigenvalue. The crucial property we will prove and use is that positive semi-definite matrices, which $X^TX$ is, have non-negative eigenvalues.\n",
    "\n",
    "Let $A$ be a positive semidefinite matrix. By definition, for any non-zero vector $x \\in \\mathbb{R}^n$, we have:\n",
    "$$\n",
    "x^T A x \\geq 0.\n",
    "$$\n",
    "\n",
    "Let $\\lambda$ be an eigenvalue of $A$ and $v$ be the corresponding eigenvector. Then:\n",
    "$$\n",
    "A v = \\lambda v\n",
    "$$\n",
    "\n",
    "Multiply both sides by $v^T$:\n",
    "$$\n",
    "v^T A v = v^T (\\lambda v) = \\lambda v^T v.\n",
    "$$\n",
    "\n",
    "Since $v$ is an eigenvector, $v \\neq 0$, and thus $v^T v > 0$. Therefore:\n",
    "$$\n",
    "v^T A v = \\lambda v^T v.\n",
    "$$\n",
    "\n",
    "Because $A$ is positive semidefinite, $v^T A v \\geq 0$. Substituting, we get:\n",
    "$$\n",
    "\\lambda v^T v \\geq 0.\n",
    "$$\n",
    "\n",
    "Since $v^T v > 0$, $\\lambda$ must also be non-negative, thereby proving the desired property.\n",
    "\n",
    "$X^TX$ is a symmetric matrix, which according the Spectral Theorem can be decomposed into $Q\\Lambda Q^T$, where $\\Lambda$ is a diagonal matrix of its eigenvalues, which we proved to be all non-negative, and $Q^T = Q^{-1}$. As we said, to make X^TX invertible, we must make all its eigenvalues non-zero, and since they are all non-negative, we should only take care of zero eigenvalues. \n",
    "\n",
    "Take a diagonal matrix $\\tilde{\\lambda}I$, where $\\tilde{\\lambda}$ an arbitrary positive number $\\tilde{\\lambda}$, which can be rewritten as $\\tilde{\\lambda}I = \\tilde{\\lambda}QQ^{-1} = {\\lambda}QQ^T $, where $Q$ is the same as above. Now, adding it to the matrix of interest:\n",
    "\n",
    "$$\n",
    "X^TX + \\tilde{\\lambda}I = Q\\Lambda Q^T + \\tilde{\\lambda}QQ^T = Q (\\Lambda + \\tilde{\\lambda}I) Q^T\n",
    "$$\n",
    "\n",
    "Since $\\tilde{\\lambda}$ is positive and the diagonal values of $\\Lambda$ are all non negative, $\\Lambda + \\tilde{\\lambda}I$ must be a diagonal matrix of all positive values, thus the matrix $X^TX + \\tilde{\\lambda}I$ is invertible.\n",
    "\n",
    "It turns out that the new matrix is an analytical solution for the L2 regularization problem:\n",
    "\n",
    "$$\n",
    "\\hat{\\beta} = \\arg\\min_{\\hat{\\beta}} (\\|Y - X \\hat{\\beta} \\|_2^2 + \\lambda^2 \\| \\hat{\\beta} \\|_2^2)\n",
    "$$\n",
    "\n",
    "which can be derived in a way analogous to that in the *Analytical solution* section."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
