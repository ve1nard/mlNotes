{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **mean square error (MSE) loss** in linear regression can be interpreted as a specific implementation of **maximum likelihood estimation (MLE)** under the assumption that the error term $ \\epsilon $ follows a **normal distribution**. \n",
    "\n",
    "#### 1. Linear Regression Model\n",
    "The linear regression model is typically written as:\n",
    "\n",
    "$$\n",
    "Y = X\\beta + \\epsilon\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ Y $ is the dependent variable (observed outcomes),\n",
    "- $ X $ is the matrix of independent variables (predictors),\n",
    "- $ \\beta $ is the vector of coefficients (parameters to be estimated),\n",
    "- $ \\epsilon $ is the error term (assumed to be independent and identically distributed).\n",
    "\n",
    "#### 2. Assumption of Normality for $ \\epsilon $\n",
    "To connect MSE to MLE, assume that:\n",
    "- The error term $ \\epsilon \\sim \\mathcal{N}(0, \\sigma^2) $ is normally distributed with mean 0 and variance $ \\sigma^2 $.\n",
    "\n",
    "This implies that $ Y | X \\sim \\mathcal{N}(X\\beta, \\sigma^2) $, meaning the conditional distribution of $ Y $ given $ X $ is normally distributed with mean $ X\\beta $ and variance $ \\sigma^2 $.\n",
    "\n",
    "#### 3. Maximum Likelihood Estimation (MLE)\n",
    "In MLE, we aim to find the parameter $ \\beta $ that maximizes the likelihood of the observed data.\n",
    "\n",
    "The likelihood function for the data $ \\{Y_i, X_i\\}_{i=1}^n $ is:\n",
    "\n",
    "$$\n",
    "L(\\beta, \\sigma^2) = \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(Y_i - X_i\\beta)^2}{2\\sigma^2}\\right)\n",
    "$$\n",
    "\n",
    "Taking the natural logarithm to simplify (log-likelihood):\n",
    "\n",
    "$$\n",
    "\\ell(\\beta, \\sigma^2) = -\\frac{n}{2} \\log(2\\pi) - \\frac{n}{2} \\log(\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^n (Y_i - X_i\\beta)^2\n",
    "$$\n",
    "\n",
    "#### 4. Maximizing the Log-Likelihood\n",
    "To maximize the log-likelihood with respect to $ \\beta $:\n",
    "- The terms $ -\\frac{n}{2} \\log(2\\pi) $ and $ -\\frac{n}{2} \\log(\\sigma^2) $ are constants with respect to $ \\beta $, so they can be ignored.\n",
    "- The remaining term to maximize (or equivalently minimize its negative) is:\n",
    "\n",
    "$$\n",
    "\\frac{1}{2\\sigma^2} \\sum_{i=1}^n (Y_i - X_i\\beta)^2\n",
    "$$\n",
    "\n",
    "Minimizing this is equivalent to minimizing the sum of squared errors (SSE):\n",
    "\n",
    "$$\n",
    "\\text{SSE} = \\sum_{i=1}^n (Y_i - X_i\\beta)^2\n",
    "$$\n",
    "\n",
    "Dividing by $ n $ gives the mean squared error (MSE):\n",
    "\n",
    "$$\n",
    "\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^n (Y_i - X_i\\beta)^2\n",
    "$$\n",
    "\n",
    "Thus, minimizing the MSE corresponds to maximizing the likelihood under the assumption of normally distributed errors."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
